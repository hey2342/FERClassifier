00~~~.sh   エンコーダのベースを学習
01~~~.sh   エンコーダをBU3DFEでファインチューニング
02~~~.sh   学習時の人物の無表情の特徴量を抽出
03~~~.sh   テスト画像の特徴抽出
04~~~.sh   異常検出と表情の有無の分類

<<<<<<< HEAD
=======


>>>>>>> 7cae96fb4570555515319141ee48cb7c26ec1ae1
テストだけなら
1. ListImages/Test_sample_youtube.txt と同じようにリストを作成
2. <path/to/dataset>/data/顔画像
                    /np/ランドマーク
                    /leye/左目画像
                    /reye/右目画像
                    /mouth/口画像
<<<<<<< HEAD
         の形でテスト用データを配置
3. 03_FeatureExtraction.sh で特徴抽出
4. 04_Eval.sh で評価
    test_label.npyに動画中でどのフレームを使うかが入ってる
        (手元の動画で表情有りのフレームが多かったので調整してるだけ)
        (-1,0,1のnumpy配列、-1は使わない0と1は使う、特になにもしない場合は動画の総フレーム数分1を並べた配列を読ませればok)

=======
   の形でテスト用データを配置
3. 03_FeatureExtraction.sh で特徴抽出
4. 04_Eval.sh で評価
    test_label.npyに動画中で使用するフレームが入ってる
        (手元の動画で表情有りのフレームが多かったので調整してるだけ)
        (-1,0,1のnumpy配列、-1は使わない0と1は使う、特になにもしない場合は動画の総フレーム数分1を並べた配列を読ませればok)
>>>>>>> 7cae96fb4570555515319141ee48cb7c26ec1ae1
