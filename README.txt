00_TrainBase.sh            エンコーダのベースを学習
01_TrainClassifier.sh      エンコーダをBU3DFEで学習
02_NeutralFeature.sh       学習時の人物の無表情の特徴量を抽出
03_FeatureExtraction.sh    テスト画像の特徴抽出
04_Eval.sh                 異常検出と表情の有無の分類


テストしたい場合
0.元画像を正面化し、目、口の画像とランドマークを用意してください。
　 画像はpngファイル、ランドマークはnumpyファイル(3次元、68点)で用意してください。
   元画像がhoge.pngの場合、正面化した各画像はhoge.png、ランドマークはhoge.npyという風に同じ名前にしてください。
    <path/to/dataset/>movie/元画像    (動画を画像にしたもの)
                  |-  data/顔画像     (正面化する前の顔画像、使わないのでなくても大丈夫です)
                  |-  np/ランドマーク    (正面化したランドマーク)
                  |-  leye/左目画像    (正面化した左目画像)
                  |-  reye/右目画像    (正面化した右目画像)
                  |-  mouth/口画像     (正面化した口画像)
  

1. ListImages/Test_sample_youtube.txt と同じようにリストを作成
    上で用意した目、口の画像、ランドマークを読み込んで特徴抽出するためのリストです。
    例えば、無表情顔のhoge.pngを読み込みたい場合
        <path/to/dataset/>data/hoge.png 0
    と書きます。表情がある場合ラベルは0ではなく1です。
    テスト時はラベルが基本的にないので全部0or1でも大丈夫です。
2. 03_FeatureExtraction.sh で特徴抽出。
    forループの中身は以下のような形になってます
        python Encoder test <1で用意したリスト> -c <学習済みのモデル> -f <特徴量の出力先>
    1で用意したリストの名前を変えたり、特徴量の出力先を変えたい場合は適宜入力の名前を変えてください。
3. 04_Eval.sh で表情表出の検出
    2で用意した特徴量を使って表情の2クラス識別をします。
    
    AnomalyDetection.py は外れ値検出で学習した特徴抽出器の数だけ表情の2クラス識別を行います。
        python AnomalyDetection.py <閾値1> <結果の出力先>
    閾値1はデータ全体で表情の出ているものが何割含まれていることにして表情の有無を推定するかを表します。
    たとえば05にすると5%、20にすると20%です。
    この数字が高いほど表情の識別がゆるくなるので、表情が出ている画像を拾いやすくなる代わりに、無表情の画像を表情有りに間違えるケースも増えます。
    第二引数は出力先のフォルダです。
    この下に prediction_<閾値>/ という名前のフォルダが作られ、その下に特徴抽出器の数だけ2クラス識別の結果がnumpyファイルで出力されます。
    
    Classifier.py は先ほどの2クラス識別結果を集計して最終的な表情の2クラス識別を行います。
        python Classifier.py -ts <閾値2> -to <閾値1> -f <テスト画像の特徴量フォルダ> -in <path/to/dataset/> -m <path/to/dataset/>movie/ -l <テスト画像の正解ラベル>
    閾値2は1の出力結果を何人分使うかを表します。デフォルトの0は全員という意味で、基本的にそのままで良いです。
    閾値1は先ほどと一緒の値にしてください。
    テスト画像の特徴量のフォルダは2で出力したものと同じにしてください。
    <path/to/dataset/>は適宜変えてください。
    テスト画像の正解ラベルは、正解ラベルがあればnumpyファイルの形式で入力してください。なければ-l以降を消してください。
    
    
    
モデルを追加したい場合
[MUG dataset](https://mug.ee.auth.gr/fed/)
[BU3DFE database](http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html)
を使いました。
BU3DFEのF0001〜F0020とM0001〜M0020の人物については学習済みです。
それ以外の人を使いたい場合だけ、読んでください。
0. 00_TrainBase.shでベースの学習 (classifier_baseがこれの出力なので無視しても良いです) 
    正面向きの表情がはっきり出ている画像で無表情とそれ以外の2クラス識別を学習します。
    MUG dataset を使いました。
    入力ファイル名とその内容(ListImages/Train_001.txtなど)は必要に応じて変えてください。
1. 01_TrainClassifier.shで学習
    0で学習したモデルをベースにBU3DFEで学習します。
    BU3DFE database を使いました。
    こちらも適宜入力等変えてください、デフォルトのものは既に学習済みです。
2. 02_NeutralFeature.shで無表情の顔画像の特徴抽出
    1で学習したモデルを使って学習に用いた同一人物の無表情の顔画像の特徴抽出を行います。
    1と同様にデフォルトのものは既に特徴抽出までできてます。
